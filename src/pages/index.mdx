---
layout: ../layouts/Layout.astro
title: An Optimisation Framework for Unsupervised Environment Design
description: Our work on creating a Unsupervised Environment Design algorithm that starts with theoretical convergence properties and builds outewards from there.
favicon: favicon.svg
thumbnail: screenshot-dark.png
---

import Header from "../components/Header.astro";
import Video from "../components/Video.astro";
import HighlightedSection from "../components/HighlightedSection.astro";
import BlueHighlightedSection from "../components/BlueHighlightedSection.astro";
import SmallCaps from "../components/SmallCaps.astro";
import Figure from "../components/Figure.astro";
import Image from "../components/Image.astro";
import TwoColumns from "../components/TwoColumns.astro";
import YouTubeVideo from "../components/YouTubeVideo.astro";
import LaTeX from "../components/LaTeX.astro";
import PDF from "../components/PDF.astro";

import { ImageComparison } from "../components/ImageComparison.tsx";

import cvar2 from "../assets/cvar_line.svg";
import ncc from "../assets/ncc.svg";
import comparison from "../assets/ncc_comparison.svg";
import hist from "../assets/hist_succ.svg";

import CodeBlock from "../components/CodeBlock.astro";
import Table from "../components/Table.astro";
export const components = {pre: CodeBlock, table: Table}

<Header
  title={frontmatter.title}
  authors={[
    {
      name: "Nathan Monette",
      url: "https://nmonette.github.io",
      institution: "UC Irvine",
      notes: ["*", "†"],
    },
    {
      name: "Alistair Letcher",
      url:"https://aletcher.github.io/",
      institution: "University of Oxford",
    },
    {
      name: "Michael Beukman",
      url:"https://michaelbeukman.com/",
      institution: "University of Oxford",
    },
    {
      name: "Matthew T. Jackson",
      url:"https://matthewtjackson.com/",
      institution: "University of Oxford",
    },
    {
      name: "Alexander Rutherford",
      url:"https://amacrutherford.github.io/",
      institution: "University of Oxford",
    },
    {
      name: "Alexander D. Goldie",
      url:"https://scholar.google.com/citations?user=wogOjBsAAAAJ&hl=en",
      institution: "University of Oxford",
    },
    {
      name: "Jakob N. Foerster",
      url: "https://www.jakobfoerster.com/",
      institution: "University of Oxford",
    },
  ]}
  conference="Reinforcement Learning Conference 2025"
  notes={[
    {
      symbol: "*",
      text: "Work undertaken while visiting the University of Oxford",
    },
    {
      symbol: "†",
      text: "Correspondence to nmonette at uci dot edu",
    },
  ]}
  links={[
    {
      name: "Paper",
      url: "https://openreview.net/pdf?id=WnknYUybWX",
      icon: "ri:file-pdf-2-line",
    },
    {
      name: "Code",
      url: "https://github.com/nmonette/NCC-UED",
      icon: "ri:github-line",
    },
  ]}
  />

## TL;DR
We take inspiration from both replay methods and generative methods for unsupervised environment design (UED), and 
thus establish a method that learns the replay distribution with gradients. From this we obtain theoretical 
convergence guarantees under light assumptions on the neural architecture, and then extend our method to have 
strong performance on current UED benchmarks.

<HighlightedSection>
## Abstract

For reinforcement learning agents to be deployed in high-risk settings, they must achieve a high level of robustness to unfamiliar scenarios. One method for improving robustness is unsupervised environment design (UED), a suite of methods aiming to maximise an agent's generalisability across configurations of an environment. In this work, we study UED from an optimisation perspective, providing stronger theoretical guarantees for practical settings than prior work. Whereas previous methods relied on guarantees *if* they reach convergence, our framework employs a nonconvex-strongly-concave objective for which we provide a *provably convergent* algorithm in the zero-sum setting. We empirically verify the efficacy of our method, outperforming prior methods on two of three environments with varying difficulties.
</HighlightedSection>

## Motivation
In its current state, UED is a very _applied_ field of research. While many of the ideas discussed in the foundational UED works like 
<a href="https://arxiv.org/abs/2012.02096">PAIRED</a> are founded in game theoretic reasoning, in practice their use of neural networks
for both the agent and the adversary creates an objective that is hard to analyse (theoretically similar to the 
<a href="https://arxiv.org/abs/1406.2661">GAN</a>). Our goal in making this paper was to create a UED method that finally has guaranteed 
convergence, and then build out from there.

<BlueHighlightedSection>
<center>_Why do we care about a theoretically convergent algorithm if the practical version doesn't converge?_</center>
</BlueHighlightedSection>


## Robustness Results
On the <a href="https://arxiv.org/abs/2306.13831">Minigrid</a> benchmark, we have access to a regret oracle. For this setting, our practical 
(not even provably convergent) method manages to strongly outperform any of our baselines!

<Figure>
  <Image slot="figure" source={cvar2} altText="Our alpha-CvAR results" width="150px" height="150px"/>
  <span slot="caption">Our <LaTeX inline formula="\alpha"/>-CVaR evaluation on the Minigrid Benchmark.</span>
</Figure>

## Method
Simply put, our algorithmic design is a hybrid of <a href="https://arxiv.org/abs/2010.03934">PLR</a>, which curates a distribution of levels to be replayed,
and <a href="https://arxiv.org/abs/2012.02096">PAIRED</a>, which learns a level generator. The way we do this is by _optimising_ the replay distribution with 
gradients!

<Figure>
  <Image slot="figure" source={ncc} altText="A diagram of our method's training loop." width="150px" height="150px"/>
  <span slot="caption">Our method's training loop, which involves gradient optimisation for both the agent and adversary.</span>
</Figure>

We then extend our method to include a dynamic level buffer similar to <a href="https://arxiv.org/abs/2010.03934">PLR</a>, and also use more contemporary
model-free RL methods standard to UED (like <a href="https://arxiv.org/abs/1707.06347">PPO</a>).

## Analysis
We provide theoretical and empirical analysis throughout our paper, with regards to convergence, as well as practicalities such as 
the choice of score function.

### Theoretical vs. Practical
We provide a formal proof for our rate of convergence under mild assumptions for the neural architecture. 
When following the theoretical regime, our method obtains suboptimal performance, but decent!
<Figure>
  <Image slot="figure" source={comparison} altText="A comparison of performance of different types of our algorithm." width="10px" height="10px"/>
  <span slot="caption">Minigrid performance with different versions of our method (NCC-T is the theoretical version and NCC-Learn/NCC-Reg are the practical variants with learnability and regret respectively).</span>
</Figure>

### A new score function
<a href="https://arxiv.org/abs/2408.15099">SFL</a> analysed different level _score_ functions, and determined that _learnability_, otherwise known 
as the variance of level-success in a goal-based environment, as being better for learning than regret approximations. We generalise this notion to the variance of returns of 
a deterministic environment, however we scale this with a gaussian to help focus on levels of intermediate difficulty. We repeat the analysis of SFL 
below on this new score function, _generalised learnability_.

<Figure>
  <Image slot="figure" source={hist} altText="A histogram that compares our score function and solve rate" width="10px" height="10px"/>
  <span slot="caption">A comparison between our new score function (black) and solve rates on Minigrid (blue).</span>
</Figure>

## What's Next?
We are excited to see any further work building off of our work! Some research directions include using bilevel optimisation theoretically 
to analyse our setting under any score function, as well as analysing more practical variants of our method, such as with better gradient 
estimators than <a href="https://link.springer.com/article/10.1007/BF00992696">REINFORCE</a>, or to handle nonstationarity with a dynamic 
buffer. Please don't hestitate to reach out if you have any questions!


## BibTeX citation

```bibtex
@inproceedings{
  monette2025an,
  title={An Optimisation Framework for Unsupervised Environment Design},
  author={Nathan Monette and Alistair Letcher and Michael Beukman and Matthew Thomas Jackson and Alexander Rutherford and Alexander David Goldie and Jakob Nicolaus Foerster},
  booktitle={Reinforcement Learning Conference},
  year={2025},
  url={https://openreview.net/forum?id=WnknYUybWX}
}
```